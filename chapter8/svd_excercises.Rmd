---
title: "SVD Excercises"
output: html_notebook
---

* http://genomicsclass.github.io/book/pages/svd.html
* http://genomicsclass.github.io/book/pages/svd_exercises.html

```{r load}
library(tissuesGeneExpression)
data(tissuesGeneExpression)
```

1. Computer the svd of e

```{r}
s = svd(e)
```

compute the row means of e

```{r}
rm = rowMeans(e)
```

what's the correlation of the first column of `U` and `rm`? 

```{r}
cor(s$u[,1], rm)
```

If we change these means, the distances between columns do not change. For example, changing the means does not change the distances:

```{r}
newmeans = rnorm(nrow(e)) #gen some new values
newe = newmeans + e
sqrt(crossprod(e[,3]-e[,45])) #distance calc for two columns in e
sqrt(crossprod(newe[,3]-newe[,45])) #distance calc for two columns in newe
```

Also note that this works b/c the difference doesn't change. 

```{r}
diff_e = e[,3]-e[,45]
diff_ne = newe[,3]-newe[,45]
diff_e[0:20] - diff_ne[0:20]  #
```

So we might as well make the mean of each row 0, since it does not help us approximate the column distances. We will define y as the *detrended* e and recompute the SVD:

```{r}
y = e - rowMeans(e)
s = svd(y)
```

We showed that `UDV⊤` is equal to y up to numerical error:

```{r}
resid = y - s$u %*% diag(s$d) %*% t(s$v)
```

The above can be made more efficient in two ways. First, using the crossprod and, second, not creating a diagonal matrix. In R, we can multiply matrices x by vector a. The result is a matrix with rows i equal to `x[i,]*a[i]`.

This means that we don’t have to convert s$d into a matrix.

2. Which of the following gives us the same as `diag(s$d)%*%t(s$v)` ?

A) `s$d %*% t(s$v)`
B) `s$d * t(s$v)`
C) `t(s$d * s$v)`
D) `s$v * s$d`


```{r}
orig = diag(s$d)%*%t(s$v)
ansB =  s$d * t(s$v)
```

3. If we define `vd = t(s$d * t(s$v))`, then which of the following is not the same UDV⊤ ?

A) `tcrossprod(s$u,vd)`
B) `s$u %*% s$d * t(s$v)`
C) `s$u %*% (s$d * t(s$v) )`
D) `tcrossprod( t( s$d*t(s$u)) , s$v)`

```{r}
vd = t(s$d * t(s$v))
a3 = tcrossprod(s$u,vd) #tcrossprod transposes the second argument instead of the first. 
dim(a3)
```
```{r}
b3 = s$u %*% s$d * t(s$v)
```


```{r}
c3= s$u %*% (s$d * t(s$v) )
dim(c3)

```
```{r}
d3 = tcrossprod( t( s$d*t(s$u)) , s$v)
dim(d3)
```

4. Let `z = s$d * t(s$v)`. We showed derivation demonstrating that because `U` is orthogonal, the distance between `e[,3]` and `e[,45]` is the same as the distance between `y[,3]` and `y[,45]` (`e` after subtracting out the rowMeans) . which is the same as `vd[,3]` and `vd[,45]` (I believe there is a typo here)



```{r}
# Which version of `s` are we talking about? Let's assume
s = svd(e)

vd = t(s$d * t(s$v))

vd_dist = sqrt(crossprod(vd[,3]-vd[,45])) 
e_dist  = sqrt(crossprod(e[,3]-e[,45]))
y_dist  = sqrt(crossprod(y[,3]-y[,45]))
c(vd_dist, e_dist, y_dist) #I didn't get the same answer here....

vd_dist = sqrt(crossprod(vd[3,]-vd[45,])) #once I changed this to row distances numbers matched up. 
c(vd_dist, e_dist, y_dist)
```
```{r}
z = s$d * t(s$v)
z_dist_rows = sqrt(crossprod(z[3,]-z[45,]))
z_dist_rows

z_dist_cols = sqrt(crossprod(z[,3]-z[,45]))
z_dist_cols
```

Note that the columns z have 189 entries, compared to 22,215 for e.

```{r}
dim(z)
```


What is the difference, in absolute value, between the actual distance:

```
sqrt(crossprod(e[,3]-e[,45]))
```

and the approximation using only two dimensions of `z`?

```{r}
z_dist2 = sqrt(crossprod(z[3,1:2]-z[45,1:2]))
```

TODO: Come up with an answer here

5. How many dimensions do we need to use for the approximation in exercise 4 to be within 10%?

Consider graphing the percent of variance explained by principal components here. Pick the top N that explain 90% of the variance. Does that get you the right answer? 


